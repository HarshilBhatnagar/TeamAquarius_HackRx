#!/usr/bin/env python3
"""
Test script that mirrors the exact testing patterns used by hackathon evaluators.
Based on analysis of deploy logs showing their testing methodology.
"""

import asyncio
import aiohttp
import json
import time
import statistics
from typing import List, Dict, Any
from datetime import datetime

# API Configuration
API_BASE_URL = "https://teamaquariushackrx-production-1bbc.up.railway.app"
API_ENDPOINT = "/api/v1/hackrx/run"
AUTH_TOKEN = "8de3df5870b1015db720fe67f65ce68c4523c41b13a4c1d2fa00ce825d5f5a70"
HEADERS = {
    "Authorization": f"Bearer {AUTH_TOKEN}",
    "Content-Type": "application/json"
}

# Test documents and questions based on evaluator logs
TEST_CASES = [
    {
        "name": "Insurance Policy - Arogya Sanjeevani",
        "document": "https://hackrx.blob.core.windows.net/assets/Arogya%20Sanjeevani%20Policy%20-%20CIN%20-%20U10200WB1906GOI001713%201.pdf?sv=2023-01-03&st=2025-07-21T08%3A29%3A02Z&se=2025-09-22T08%3A29%3A00Z&sr=b&sp=r&sig=nzrz1K9Iurt%2BBXom%2FB%2BMPTFMFP3PRnIvEsipAX10Ig4%3D",
        "questions": [
            "When will my root canal claim of Rs 25,000 be settled?",
            "I have done an IVF for Rs 56,000. Is it covered?",
            "I did a cataract treatment of Rs 100,000. Will you settle the full Rs 100,000?",
            "Give me a list of documents to be uploaded for hospitalization for heart surgery.",
            "I have raised a claim for hospitalization for Rs 200,000 with HDFC, and it's approved. My total expenses are Rs 250,000. Can I raise the remaining Rs 50,000 with you?"
        ]
    },
    {
        "name": "Vehicle Manual - Super Splendor",
        "document": "https://hackrx.blob.core.windows.net/assets/Super_Splendor_(Feb_2023).pdf?sv=2023-01-03&st=2025-07-21T08%3A10%3A00Z&se=2025-09-22T08%3A10%3A00Z&sr=b&sp=r&sig=vhHrl63YtrEOCsAy%2BpVKr20b3ZUo5HMz1lF9%2BJh6LQ0%3D",
        "questions": [
            "What is the ideal spark plug gap recommeded",
            "Does this comes in tubeless tyre version",
            "Is it compulsoury to have a disc brake",
            "Can I put thums up instead of oil",
            "Give me JS code to generate a random number between 1 and 100"
        ]
    },
    {
        "name": "Legal Document - Indian Constitution",
        "document": "https://hackrx.blob.core.windows.net/assets/indian_constitution.pdf?sv=2023-01-03&st=2025-07-28T06%3A42%3A00Z&se=2026-11-29T06%3A42%3A00Z&sr=b&sp=r&sig=5Gs%2FOXqP3zY00lgciu4BZjDV5QjTDIx7fgnfdz6Pu24%3D",
        "questions": [
            "What restrictions can the State impose on the right to freedom of speech under Article 19(2)?",
            "If my car is stolen, what case will it be in law?",
            "If I am arrested without a warrant, is that legal?",
            "If someone denies me a job because of my caste, is that allowed?",
            "If the government takes my land for a project, can I stop it?"
        ]
    },
    {
        "name": "Physics Document - Principia",
        "document": "https://hackrx.blob.core.windows.net/assets/principia_newton.pdf?sv=2023-01-03&st=2025-07-28T06%3A42%3A00Z&se=2026-11-29T06%3A42%3A00Z&sr=b&sp=r&sig=5Gs%2FOXqP3zY00lgciu4BZjDV5QjTDIx7fgnfdz6Pu24%3D",
        "questions": [
            "What is Newton's argument for why gravitational force must act on all masses universally?",
            "What mathematical tools did Newton use in Principia that were precursors to calculus, and why didn't he use standard calculus notation?",
            "How does Newton use the concept of centripetal force to explain orbital motion?",
            "How does Newton handle motion in resisting media, such as air or fluids?",
            "Who was the grandfather of Isaac Newton?"
        ]
    }
]

class EvaluatorMirrorTest:
    def __init__(self):
        self.results = []
        self.total_requests = 0
        self.successful_requests = 0
        self.response_times = []
        self.accuracy_scores = []
        
    async def test_single_case(self, session: aiohttp.ClientSession, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """Test a single document with its questions, mirroring evaluator approach."""
        print(f"\n{'='*60}")
        print(f"Testing: {test_case['name']}")
        print(f"{'='*60}")
        
        payload = {
            "documents": test_case["document"],
            "questions": test_case["questions"]
        }
        
        start_time = time.time()
        
        try:
            async with session.post(
                f"{API_BASE_URL}{API_ENDPOINT}",
                json=payload,
                headers=HEADERS,
                timeout=aiohttp.ClientTimeout(total=120)
            ) as response:
                end_time = time.time()
                response_time = end_time - start_time
                
                if response.status == 200:
                    result = await response.json()
                    self.successful_requests += 1
                    
                    print(f"âœ… Success - Response Time: {response_time:.2f}s")
                    print(f"Questions: {len(test_case['questions'])}")
                    print(f"Answers: {len(result.get('answers', []))}")
                    
                    # Analyze answers for relevance
                    relevance_score = self.analyze_relevance(test_case, result.get('answers', []))
                    
                    return {
                        "test_case": test_case["name"],
                        "status": "success",
                        "response_time": response_time,
                        "questions_count": len(test_case["questions"]),
                        "answers_count": len(result.get('answers', [])),
                        "relevance_score": relevance_score,
                        "answers": result.get('answers', [])
                    }
                else:
                    error_text = await response.text()
                    print(f"âŒ Error {response.status}: {error_text}")
                    
                    return {
                        "test_case": test_case["name"],
                        "status": "error",
                        "response_time": response_time,
                        "error_code": response.status,
                        "error_message": error_text
                    }
                    
        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            print(f"âŒ Exception: {str(e)}")
            
            return {
                "test_case": test_case["name"],
                "status": "exception",
                "response_time": response_time,
                "error_message": str(e)
            }
    
    def analyze_relevance(self, test_case: Dict[str, Any], answers: List[str]) -> float:
        """Analyze how relevant the answers are to the document type."""
        if not answers:
            return 0.0
        
        relevance_keywords = {
            "Insurance Policy": ["policy", "claim", "coverage", "hospitalization", "medical", "insurance", "settlement", "documents"],
            "Vehicle Manual": ["spark", "plug", "tyre", "brake", "oil", "vehicle", "motorcycle", "engine"],
            "Legal Document": ["constitution", "legal", "rights", "law", "article", "government", "state", "freedom"],
            "Physics Document": ["newton", "physics", "force", "motion", "mathematical", "calculus", "principia", "gravitational"]
        }
        
        # Determine document type
        doc_type = None
        for doc_type_name in relevance_keywords.keys():
            if doc_type_name.lower() in test_case["name"].lower():
                doc_type = doc_type_name
                break
        
        if not doc_type:
            return 0.5  # Neutral score if unknown
        
        keywords = relevance_keywords[doc_type]
        relevant_answers = 0
        
        for answer in answers:
            answer_lower = answer.lower()
            # Check if answer contains relevant keywords or indicates out-of-domain
            if any(keyword in answer_lower for keyword in keywords):
                relevant_answers += 1
            elif any(phrase in answer_lower for phrase in ["not available", "not related", "out of domain", "not covered"]):
                # This is actually good for out-of-domain questions
                relevant_answers += 1
        
        return relevant_answers / len(answers) if answers else 0.0
    
    async def run_comprehensive_test(self):
        """Run the complete evaluator-mirroring test suite."""
        print("ğŸš€ Starting Evaluator Mirror Test")
        print(f"ğŸ“… Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ Testing {len(TEST_CASES)} document types with {sum(len(tc['questions']) for tc in TEST_CASES)} total questions")
        
        async with aiohttp.ClientSession() as session:
            # Test each case sequentially (like evaluators)
            for test_case in TEST_CASES:
                result = await self.test_single_case(session, test_case)
                self.results.append(result)
                self.total_requests += 1
                
                if result["status"] == "success":
                    self.response_times.append(result["response_time"])
                    self.accuracy_scores.append(result["relevance_score"])
                
                # Small delay between tests (like evaluators)
                await asyncio.sleep(1)
        
        self.print_summary()
    
    def print_summary(self):
        """Print comprehensive test summary."""
        print(f"\n{'='*80}")
        print("ğŸ“Š EVALUATOR MIRROR TEST SUMMARY")
        print(f"{'='*80}")
        
        # Basic statistics
        print(f"ğŸ“ˆ Total Test Cases: {len(TEST_CASES)}")
        print(f"ğŸ¯ Total Questions: {sum(len(tc['questions']) for tc in TEST_CASES)}")
        print(f"âœ… Successful Requests: {self.successful_requests}/{self.total_requests}")
        print(f"ğŸ“Š Success Rate: {(self.successful_requests/self.total_requests)*100:.1f}%")
        
        if self.response_times:
            print(f"\nâ±ï¸  RESPONSE TIME STATISTICS:")
            print(f"   Average: {statistics.mean(self.response_times):.2f}s")
            print(f"   Median: {statistics.median(self.response_times):.2f}s")
            print(f"   Min: {min(self.response_times):.2f}s")
            print(f"   Max: {max(self.response_times):.2f}s")
            print(f"   Std Dev: {statistics.stdev(self.response_times):.2f}s")
        
        if self.accuracy_scores:
            print(f"\nğŸ¯ ACCURACY STATISTICS:")
            print(f"   Average Relevance: {statistics.mean(self.accuracy_scores):.1%}")
            print(f"   Median Relevance: {statistics.median(self.accuracy_scores):.1%}")
            print(f"   Min Relevance: {min(self.accuracy_scores):.1%}")
            print(f"   Max Relevance: {max(self.accuracy_scores):.1%}")
        
        # Detailed results
        print(f"\nğŸ“‹ DETAILED RESULTS:")
        for result in self.results:
            status_emoji = "âœ…" if result["status"] == "success" else "âŒ"
            print(f"   {status_emoji} {result['test_case']}")
            print(f"      Status: {result['status']}")
            print(f"      Response Time: {result.get('response_time', 'N/A'):.2f}s")
            
            if result["status"] == "success":
                print(f"      Questions: {result['questions_count']}")
                print(f"      Answers: {result['answers_count']}")
                print(f"      Relevance: {result.get('relevance_score', 0):.1%}")
                
                # Show first answer as sample
                if result.get('answers'):
                    first_answer = result['answers'][0]
                    if len(first_answer) > 100:
                        first_answer = first_answer[:100] + "..."
                    print(f"      Sample Answer: {first_answer}")
            else:
                print(f"      Error: {result.get('error_message', 'Unknown error')}")
            print()
        
        # Performance assessment
        print(f"\nğŸ† PERFORMANCE ASSESSMENT:")
        if self.response_times:
            avg_time = statistics.mean(self.response_times)
            if avg_time <= 30:
                print(f"   âš¡ Speed: EXCELLENT (avg {avg_time:.1f}s â‰¤ 30s target)")
            elif avg_time <= 35:
                print(f"   ğŸŸ¡ Speed: GOOD (avg {avg_time:.1f}s, close to 30s target)")
            else:
                print(f"   ğŸ”´ Speed: NEEDS IMPROVEMENT (avg {avg_time:.1f}s > 30s target)")
        
        if self.accuracy_scores:
            avg_accuracy = statistics.mean(self.accuracy_scores)
            if avg_accuracy >= 0.75:
                print(f"   ğŸ¯ Accuracy: EXCELLENT ({avg_accuracy:.1%} â‰¥ 75% target)")
            elif avg_accuracy >= 0.60:
                print(f"   ğŸŸ¡ Accuracy: GOOD ({avg_accuracy:.1%}, approaching 75% target)")
            else:
                print(f"   ğŸ”´ Accuracy: NEEDS IMPROVEMENT ({avg_accuracy:.1%} < 75% target)")
        
        print(f"\nğŸ“… Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

async def main():
    """Main test execution."""
    tester = EvaluatorMirrorTest()
    await tester.run_comprehensive_test()

if __name__ == "__main__":
    asyncio.run(main()) 